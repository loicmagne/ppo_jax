{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T5nzhpM9ikMZ"
      },
      "source": [
        "# Installs & Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Prf32ZlzaieY"
      },
      "outputs": [],
      "source": [
        "!pip install dm-env\n",
        "!pip install dm-haiku\n",
        "!pip install dm-tree\n",
        "!pip install optax\n",
        "!pip install distrax\n",
        "!pip install chex\n",
        "!pip install imageio\n",
        "!pip install gym\n",
        "!pip install gym[classic_control]\n",
        "!pip install free-mujoco-py\n",
        "!pip install wandb\n",
        "\n",
        "!apt install -y x11-utils xvfb x11-utils ffmpeg\n",
        "!apt install -y libgl1-mesa-dev libgl1-mesa-glx libglew-dev libosmesa6-dev software-properties-common\n",
        "!apt install -y patchelf\n",
        "\n",
        "!pip install pyglet\n",
        "!pip install gym pyvirtualdisplay\n",
        "!pip install imageio-ffmpeg\n",
        "\n",
        "from IPython.display import clear_output\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dwCVzAxnilB0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b06dbb68-72b6-4235-c766-5d7df76089f7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyvirtualdisplay.display.Display at 0x7f8488194550>"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import IPython\n",
        "from IPython.display import HTML\n",
        "from IPython import display as ipythondisplay\n",
        "\n",
        "# Python stuff\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# RL stuff\n",
        "import dm_env\n",
        "import gym\n",
        "import mujoco_py\n",
        "\n",
        "# DL stuff\n",
        "from jax import tree_util\n",
        "import haiku as hk\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import optax\n",
        "import distrax\n",
        "\n",
        "from base64 import b64encode\n",
        "from collections import namedtuple\n",
        "from typing import *\n",
        "from tqdm import tqdm\n",
        "import base64\n",
        "import chex\n",
        "import collections\n",
        "import enum\n",
        "import functools\n",
        "import imageio\n",
        "import io\n",
        "import itertools\n",
        "import multiprocessing as mp\n",
        "import multiprocessing.connection\n",
        "import random\n",
        "import time\n",
        "import tree\n",
        "import warnings\n",
        "import wandb\n",
        "\n",
        "# gym render things, if this doesn't work for you -> good luck\n",
        "import pyglet\n",
        "pyglet.options['search_local_libs'] = False\n",
        "pyglet.options['shadow_window']=False\n",
        "from pyglet.window import xlib\n",
        "xlib._have_utf8 = False\n",
        "\n",
        "from pyvirtualdisplay import Display\n",
        "display = Display(visible=False, size=(1400, 900))\n",
        "display.start()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hWLm1ILwQVuT"
      },
      "source": [
        "# Environnement"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u70szl5IQWmx"
      },
      "outputs": [],
      "source": [
        "class InvertedPendulumEnv(dm_env.Environment):\n",
        "    def __init__(self, for_evaluation: bool) -> None:\n",
        "        self._env = gym.make('InvertedPendulum-v2')\n",
        "        self._for_evaluation = for_evaluation\n",
        "        if self._for_evaluation:\n",
        "            self.screens = []\n",
        "\n",
        "    def step(self, action: chex.ArrayNumpy) -> dm_env.TimeStep:\n",
        "        new_obs, reward, done, _ = self._env.step(action)\n",
        "        if self._for_evaluation:\n",
        "            self.screens.append(self._env.render(mode='rgb_array'))\n",
        "        if done:\n",
        "            return dm_env.termination(reward, new_obs)\n",
        "        return dm_env.transition(reward, new_obs)\n",
        "\n",
        "    def reset(self) -> dm_env.TimeStep:\n",
        "        obs = self._env.reset()\n",
        "        if self._for_evaluation:\n",
        "            self.screens.append(self._env.render(mode='rgb_array'))\n",
        "        return dm_env.restart(obs)\n",
        "\n",
        "    def observation_spec(self) -> dm_env.specs.BoundedArray:\n",
        "        return dm_env.specs.Array(shape=(4,), dtype=np.float32)\n",
        "\n",
        "    def action_spec(self) -> dm_env.specs.BoundedArray:\n",
        "        return dm_env.specs.BoundedArray(shape=(1,), minimum=-3., maximum=3., dtype=np.float32)\n",
        "\n",
        "    def close(self) -> None:\n",
        "        self._env.close()\n",
        "\n",
        "class ReacherEnv(dm_env.Environment):\n",
        "    def __init__(self, for_evaluation: bool) -> None:\n",
        "        self._env = gym.make(\"Reacher-v2\")\n",
        "        self._for_evaluation = for_evaluation\n",
        "        if self._for_evaluation:\n",
        "            self.screens = []\n",
        "\n",
        "    def step(self, action: chex.ArrayNumpy) -> dm_env.TimeStep:\n",
        "        new_obs, reward, done, _ = self._env.step(action)\n",
        "        if self._for_evaluation:\n",
        "            self.screens.append(self._env.render(mode='rgb_array'))\n",
        "        if done:\n",
        "            return dm_env.termination(reward, new_obs)\n",
        "        return dm_env.transition(reward, new_obs)\n",
        "\n",
        "    def reset(self) -> dm_env.TimeStep:\n",
        "        obs = self._env.reset()\n",
        "        if self._for_evaluation:\n",
        "            self.screens.append(self._env.render(mode='rgb_array'))\n",
        "        return dm_env.restart(obs)\n",
        "\n",
        "    def observation_spec(self) -> dm_env.specs.BoundedArray:\n",
        "        return dm_env.specs.Array(shape=(11,), dtype=np.float32)\n",
        "\n",
        "    def action_spec(self) -> dm_env.specs.BoundedArray:\n",
        "        return dm_env.specs.BoundedArray(shape=(2,), minimum=-1., maximum=1., dtype=np.float32)\n",
        "\n",
        "    def close(self) -> None:\n",
        "        self._env.close()\n",
        "\n",
        "class InvertedDoublePendulumEnv(dm_env.Environment):\n",
        "    def __init__(self, for_evaluation: bool) -> None:\n",
        "        self._env = gym.make(\"InvertedDoublePendulum-v2\")\n",
        "        self._for_evaluation = for_evaluation\n",
        "        if self._for_evaluation:\n",
        "            self.screens = []\n",
        "\n",
        "    def step(self, action: chex.ArrayNumpy) -> dm_env.TimeStep:\n",
        "        new_obs, reward, done, _ = self._env.step(action)\n",
        "        if self._for_evaluation:\n",
        "            self.screens.append(self._env.render(mode='rgb_array'))\n",
        "        if done:\n",
        "            return dm_env.termination(reward, new_obs)\n",
        "        return dm_env.transition(reward, new_obs)\n",
        "\n",
        "    def reset(self) -> dm_env.TimeStep:\n",
        "        obs = self._env.reset()\n",
        "        if self._for_evaluation:\n",
        "            self.screens.append(self._env.render(mode='rgb_array'))\n",
        "        return dm_env.restart(obs)\n",
        "\n",
        "    def observation_spec(self) -> dm_env.specs.BoundedArray:\n",
        "        return dm_env.specs.Array(shape=(11,), dtype=np.float32)\n",
        "\n",
        "    def action_spec(self) -> dm_env.specs.BoundedArray:\n",
        "        return dm_env.specs.BoundedArray(shape=(1,), minimum=-1., maximum=1., dtype=np.float32)\n",
        "\n",
        "    def close(self) -> None:\n",
        "        self._env.close()\n",
        "\n",
        "class CartPole(dm_env.Environment):\n",
        "    def __init__(self, for_evaluation: bool) -> None:\n",
        "        self._env = gym.make('CartPole-v1')\n",
        "        self._for_evaluation = for_evaluation\n",
        "        if self._for_evaluation:\n",
        "            self.screens = []\n",
        "\n",
        "    def step(self, action: chex.ArrayNumpy) -> dm_env.TimeStep:\n",
        "        new_obs, reward, done, _ = self._env.step(action)\n",
        "        if self._for_evaluation:\n",
        "            self.screens.append(self._env.render(mode='rgb_array'))\n",
        "        if done:\n",
        "            return dm_env.termination(reward, new_obs)\n",
        "        return dm_env.transition(reward, new_obs)\n",
        "\n",
        "    def reset(self) -> dm_env.TimeStep:\n",
        "        obs = self._env.reset()\n",
        "        if self._for_evaluation:\n",
        "            self.screens.append(self._env.render(mode='rgb_array'))\n",
        "        return dm_env.restart(obs)\n",
        "\n",
        "    def observation_spec(self) -> dm_env.specs.BoundedArray:\n",
        "        return dm_env.specs.Array(shape=(4,), dtype=np.float32)\n",
        "\n",
        "    def action_spec(self) -> dm_env.specs.BoundedArray:\n",
        "        return dm_env.specs.DiscreteArray(num_values=self._env.action_space.n, dtype=np.int32)\n",
        "\n",
        "    def close(self) -> None:\n",
        "        self._env.close()\n",
        "\n",
        "class TransitionBuffer():\n",
        "    def __init__(self):\n",
        "        self.action = []\n",
        "        self.action_log_prob = []\n",
        "        self.state_old = []\n",
        "        self.state_new = []\n",
        "        self.state_type_old = []\n",
        "        self.state_type_new = []\n",
        "        self.reward = []\n",
        "    def push(self, ts, next_ts, a, log_prob):\n",
        "        self.action.append(a)\n",
        "        self.action_log_prob.append(log_prob)\n",
        "        self.state_old.append(ts.observation)\n",
        "        self.state_type_old.append(ts.step_type)\n",
        "        self.state_new.append(next_ts.observation)\n",
        "        self.state_type_new.append(next_ts.step_type)\n",
        "        self.reward.append(next_ts.reward)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tWxG1UwHVqUZ"
      },
      "source": [
        "# Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mmc0kD7XVsGF"
      },
      "outputs": [],
      "source": [
        "# Parameters\n",
        "logging = True\n",
        "continuous = True\n",
        "env_factory = InvertedPendulumEnv\n",
        "iterations = 1000\n",
        "N = 5\n",
        "T = 2048\n",
        "gamma = 0.99\n",
        "lbd = 0.95\n",
        "n_epochs = 10\n",
        "batch_size = 64\n",
        "eps = 0.2 # Clip loss epsilon\n",
        "c1 = 0.9 # Value Function loss coefficient\n",
        "c2 = 0.01 # Entropy loss coefficient\n",
        "key = jax.random.PRNGKey(28042000)\n",
        "learning_rate = 3e-4\n",
        "\n",
        "# Initialize env\n",
        "env = env_factory(False)\n",
        "state_dim = env.observation_spec().shape[0]\n",
        "if not(continuous):\n",
        "    action_dim = env.action_spec().num_values\n",
        "else:\n",
        "    action_dim = env.action_spec().shape[0]\n",
        "\n",
        "# Networks\n",
        "def policy_network(x):\n",
        "    x = hk.nets.MLP([64,64,action_dim], activation=jax.numpy.tanh)(x)\n",
        "    return x\n",
        "key, subkey = jax.random.split(key)\n",
        "policy = hk.without_apply_rng(hk.transform(policy_network))\n",
        "policy_params = policy.init(rng=subkey, x=jnp.ones([1, state_dim]))\n",
        "\n",
        "def value_network(x):\n",
        "    x = hk.nets.MLP([64,64,1], activation=jax.numpy.tanh)(x)\n",
        "    return x\n",
        "key, subkey = jax.random.split(key)\n",
        "value = hk.without_apply_rng(hk.transform(value_network))\n",
        "value_params = value.init(rng=subkey, x=jnp.ones([1, state_dim]))\n",
        "\n",
        "\n",
        "# Optimizers\n",
        "policy_optimizer = optax.adam(learning_rate)\n",
        "policy_opt_state = policy_optimizer.init(policy_params)\n",
        "value_optimizer = optax.adam(learning_rate)\n",
        "value_opt_state = value_optimizer.init(value_params)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TOFmKLaaVyiC"
      },
      "source": [
        "# PPO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QlcLpU2ASeiH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "10456615-c292-4cef-b180-5f0c070a29a9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mlmagne\u001b[0m (use `wandb login --relogin` to force relogin)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.12.11"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20220401_010530-14p9lxgg</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/lmagne/ppo_jax/runs/14p9lxgg\" target=\"_blank\">dark-shadow-24</a></strong> to <a href=\"https://wandb.ai/lmagne/ppo_jax\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/jax/_src/random.py:371: FutureWarning: jax.random.shuffle is deprecated and will be removed in a future release. Use jax.random.permutation with independent=True.\n",
            "  warnings.warn(msg, FutureWarning)\n"
          ]
        }
      ],
      "source": [
        "@jax.jit\n",
        "def loss_fct(policy_params, value_params,\n",
        "             advantage, state, action, target, log_prob,\n",
        "             eps, c1, c2):\n",
        "    policy_apply_output = policy.apply(policy_params, state)\n",
        "    entropies, log_prob_new = jax.vmap(policy_entropy_logprob)(policy_apply_output, action)\n",
        "    r = jnp.exp(log_prob_new - log_prob) # Compute r ratio\n",
        "    # Compute each term\n",
        "    l_clip = jnp.minimum(r * advantage, jnp.clip(r, 1 - eps, 1 + eps) * advantage)\n",
        "    l_entropy = entropies\n",
        "    l_vf = jnp.square((value.apply(value_params, state) - target))\n",
        "    loss = jnp.mean(- l_clip + c1 * l_vf - c2 * l_entropy)\n",
        "    return loss\n",
        "\n",
        "@jax.jit\n",
        "def compute_advantage(policy_params, value_params,\n",
        "                      reward, state_old, state_new, state_type_old, state_type_new, action,\n",
        "                      gamma, lbd):\n",
        "    is_first = (state_type_old == dm_env.StepType.FIRST)\n",
        "    is_last = jnp.roll(is_first, -1)\n",
        "    target = reward + gamma * value.apply(value_params, state_new).squeeze() * (1 - is_last)\n",
        "    delta = (target - value.apply(value_params, state_old).squeeze())\n",
        "    advantage = []\n",
        "    acc = 0.\n",
        "    for d, reset in zip(delta[::-1], is_first[::-1]):\n",
        "        acc = lbd * gamma * acc + d\n",
        "        advantage.append(acc)\n",
        "        acc *= (1 - reset)\n",
        "    advantage = jnp.array(advantage[::-1])\n",
        "    advantage = (advantage - jnp.mean(advantage)) / jnp.std(advantage) # Normalize advantage\n",
        "    return target, advantage\n",
        "\n",
        "if not(continuous):\n",
        "    @jax.jit\n",
        "    def policy_entropy_logprob(policy_apply_output, action):\n",
        "        distrib = distrax.Categorical(logits=policy_apply_output)\n",
        "        entropy = distrib.entropy()\n",
        "        log_prob = distrib.log_prob(action)\n",
        "        return entropy, log_prob\n",
        "    @jax.jit\n",
        "    def policy_sample(policy_apply_output, key):\n",
        "        distrib = distrax.Categorical(logits=policy_apply_output)\n",
        "        action, log_prob = distrib.sample_and_log_prob(seed=key, sample_shape=())\n",
        "        return action.astype(int), log_prob\n",
        "else:\n",
        "    std = 0.2 * jnp.ones(action_dim)\n",
        "    @jax.jit\n",
        "    def policy_entropy_logprob(policy_apply_output, action, std=std):\n",
        "        distrib = distrax.MultivariateNormalDiag(loc=policy_apply_output, scale_diag=std)\n",
        "        entropy = distrib.entropy()\n",
        "        log_prob = distrib.log_prob(action)\n",
        "        return entropy, log_prob\n",
        "    @jax.jit\n",
        "    def policy_sample(policy_apply_output, key):\n",
        "        distrib = distrax.MultivariateNormalDiag(loc=policy_apply_output, scale_diag=std)\n",
        "        action, log_prob = distrib.sample_and_log_prob(seed=key, sample_shape=())\n",
        "        return action, log_prob\n",
        "        \n",
        "full_reward_history = []\n",
        "episode_reward_history = []\n",
        "episode_timesteps_history = []\n",
        "loss_history = []\n",
        "ts_count = 0\n",
        "if logging:\n",
        "    wandb.init(project=\"ppo_jax\",reinit=True)\n",
        "# for i in tqdm(range(iterations)):\n",
        "while ts_count < 1000000:\n",
        "    ts_buffer = TransitionBuffer()\n",
        "    # Run N episodes over T timesteps\n",
        "    for actor in range(N):\n",
        "        ts = env.reset()\n",
        "        acc_reward = 0\n",
        "        for t in range(T):\n",
        "            if ts.step_type == dm_env.StepType.LAST:\n",
        "                break\n",
        "            policy_apply_output = policy.apply(policy_params, jnp.expand_dims(ts.observation,0))\n",
        "            key, subkey = jax.random.split(key)\n",
        "            # action = int(jax.random.choice(subkey, action_dim, p=proba))\n",
        "            action, log_prob = policy_sample(jnp.squeeze(policy_apply_output,0), key)\n",
        "            next_ts = env.step(action)\n",
        "            ts_buffer.push(ts, next_ts, action, log_prob)\n",
        "            ts = next_ts\n",
        "            acc_reward += next_ts.reward\n",
        "\n",
        "            # Logging \n",
        "            full_reward_history.append(acc_reward)\n",
        "            ts_count += 1\n",
        "            if (ts_count % 1000000) == 0:\n",
        "                print(f'{ts_count} timesteps reached')\n",
        "            if (ts_count % 100000) == 0:\n",
        "                eps = eps / 2.\n",
        "        episode_reward_history.append(acc_reward)\n",
        "        episode_reward_history.append(ts_count)\n",
        "        if logging:\n",
        "            wandb.log({'episode_reward':acc_reward, 'episode_timestep':ts_count})\n",
        "\n",
        "    # Compute advantage\n",
        "    action = jnp.array(ts_buffer.action)\n",
        "    state_old = jnp.array(ts_buffer.state_old)\n",
        "    state_new = jnp.array(ts_buffer.state_new)\n",
        "    state_type_old = jnp.array(ts_buffer.state_type_old)\n",
        "    state_type_new = jnp.array(ts_buffer.state_type_new)\n",
        "    reward = jnp.array(ts_buffer.reward)\n",
        "    log_prob = jnp.array(ts_buffer.action_log_prob)\n",
        "\n",
        "    target, advantage = compute_advantage(policy_params, value_params, reward, state_old, state_new, state_type_old, state_type_new, action, gamma, lbd)\n",
        "\n",
        "    # Update parameters\n",
        "    for _ in range(n_epochs):\n",
        "        # Compute loss            \n",
        "        loss, (policy_grads, value_grads) = jax.value_and_grad(loss_fct, argnums = (0, 1))(\n",
        "            policy_params, value_params, advantage, state_old,\n",
        "            action, target, log_prob, eps, c1, c2\n",
        "        )\n",
        "\n",
        "        # Update parameters\n",
        "        updates, policy_opt_state = policy_optimizer.update(policy_grads, policy_opt_state)\n",
        "        policy_params = optax.apply_updates(policy_params, updates)\n",
        "\n",
        "        updates, value_opt_state = value_optimizer.update(value_grads, value_opt_state)\n",
        "        value_params = optax.apply_updates(value_params, updates)\n",
        "\n",
        "        loss_history.append(float(loss))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-OFDcT2vnMnh"
      },
      "source": [
        "# Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4fTspbNkaSdZ"
      },
      "outputs": [],
      "source": [
        "def moving_average(x, w):\n",
        "    return np.convolve(x, np.ones(w), 'valid') / w\n",
        "plt.figure(figsize=(8, 6), dpi=80)\n",
        "plt.plot(moving_average(full_reward_history,1000))\n",
        "plt.title('rewards')\n",
        "plt.tight_layout()\n",
        "plt.savefig('reward.png')\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(8, 6), dpi=80)\n",
        "plt.plot(episode_reward_history)\n",
        "plt.title('full episode rewards')\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(8, 6), dpi=80)\n",
        "plt.plot(loss_history)\n",
        "plt.title('loss')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rkKCHGJJnN4s"
      },
      "outputs": [],
      "source": [
        "# Initialize env\n",
        "env = env_factory(True)\n",
        "# state_dim = env.observation_spec().shape[0]\n",
        "# action_dim = env.action_spec().num_values\n",
        "\n",
        "def display_video(frames, filename='temp.mp4', frame_repeat=1):\n",
        "    \"\"\"Save and display video.\"\"\"\n",
        "    frames = np.stack(frames, axis=0)\n",
        "    # Write video\n",
        "    with imageio.get_writer(filename, fps=60) as video:\n",
        "        for frame in frames:\n",
        "            for _ in range(frame_repeat):\n",
        "                video.append_data(frame)\n",
        "        # Read video and display the video\n",
        "        video = open(filename, 'rb').read()\n",
        "        b64_video = base64.b64encode(video)\n",
        "        video_tag = ('<video  width=\"320\" height=\"240\" controls alt=\"test\" '\n",
        "                   'src=\"data:video/mp4;base64,{0}\">').format(b64_video.decode())\n",
        "    return IPython.display.HTML(video_tag)\n",
        "\n",
        "def clip(i):\n",
        "    T = 50000\n",
        "\n",
        "    ts = env.reset()\n",
        "    acc_reward = 0\n",
        "    episode_rewards = []\n",
        "    key = jax.random.PRNGKey(28042000)\n",
        "    for t in range(T):\n",
        "        if ts.step_type == dm_env.StepType.LAST:\n",
        "            break\n",
        "        print(t, ts.reward)\n",
        "        proba = policy.apply(policy_params, jnp.expand_dims(ts.observation,0))\n",
        "        key, subkey = jax.random.split(key)\n",
        "        action, log_prob = policy_sample(proba, key)\n",
        "        ts = env.step(action)\n",
        "        acc_reward += ts.reward\n",
        "        episode_rewards.append(acc_reward)\n",
        "    display_video(env.screens, f'temp{i}.mp4')\n",
        "    \n",
        "for i in range(10):\n",
        "    clip(i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OEElUk5coIxm"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "files.download('reward.png') \n",
        "files.download('temp.mp4') "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "axYld_4VhGgN"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "ppo_jax(6).ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}